{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYJLnOeXhNfJeZnStFgU3B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shotahorii/bareml/blob/master/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOD3J29D1726",
        "outputId": "15e391dc-cad1-439e-d9eb-6a0ca790bd88"
      },
      "source": [
        "# clone bareml repo\n",
        "!git clone https://github.com/shotahorii/bareml.git\n",
        "\n",
        "# move to the bareml repo\n",
        "import os\n",
        "path = '/content/bareml'\n",
        "os.chdir(path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bareml'...\n",
            "remote: Enumerating objects: 647, done.\u001b[K\n",
            "remote: Counting objects: 100% (647/647), done.\u001b[K\n",
            "remote: Compressing objects: 100% (447/447), done.\u001b[K\n",
            "remote: Total 1188 (delta 407), reused 418 (delta 187), pack-reused 541\u001b[K\n",
            "Receiving objects: 100% (1188/1188), 18.05 MiB | 17.98 MiB/s, done.\n",
            "Resolving deltas: 100% (719/719), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU2z8J9d2Amh"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "  import cupy as cp\n",
        "  device = 'cuda'\n",
        "except:\n",
        "  device = 'cpu'\n",
        "\n",
        "from bareml.deeplearning import functions as F\n",
        "from bareml.deeplearning.models import CBOW\n",
        "from bareml.deeplearning.optimisers import Adam\n",
        "from bareml.deeplearning.data import NewsGroupsWordInference, DataLoader\n",
        "from bareml.deeplearning.metrics import cos_similarity"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIQionLpcC3B"
      },
      "source": [
        "# set some parameters \n",
        "num_epoch = 10\n",
        "batch_size = 256"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWonM-s82Ic3",
        "outputId": "0bdd7da3-cc35-4f5c-d8aa-6c9b18af40ae"
      },
      "source": [
        "# load the dataset\n",
        "dataset = NewsGroupsWordInference()\n",
        "dataloader = DataLoader(dataset, batch_size)\n",
        "num_batches_per_epoch = len(dataset)//batch_size"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating NewsGroupsWordInference Dataset: Step 1/2\n",
            "Downloading: 20news-bydate.tar.gz\n",
            "[##############################] 100.00% Done\n",
            "-- Creating the corpus --\n",
            "1000/11314 docs\n",
            "2000/11314 docs\n",
            "3000/11314 docs\n",
            "4000/11314 docs\n",
            "5000/11314 docs\n",
            "6000/11314 docs\n",
            "7000/11314 docs\n",
            "8000/11314 docs\n",
            "9000/11314 docs\n",
            "10000/11314 docs\n",
            "11000/11314 docs\n",
            "Creating NewsGroupsWordInference Dataset: Step 2/2\n",
            "1000/11314 docs\n",
            "2000/11314 docs\n",
            "3000/11314 docs\n",
            "4000/11314 docs\n",
            "5000/11314 docs\n",
            "6000/11314 docs\n",
            "7000/11314 docs\n",
            "8000/11314 docs\n",
            "9000/11314 docs\n",
            "10000/11314 docs\n",
            "11000/11314 docs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvTBCCmqcXVx"
      },
      "source": [
        "# define the model and optimiser\n",
        "model = CBOW(dataset.corpus.corpus,embedding_dim=100).to(device)\n",
        "optimiser = Adam(model.parameters())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzOH4vKhcXef",
        "outputId": "dce1c1a5-766f-449d-ed7a-f6c2ffd0aab6"
      },
      "source": [
        "# train the model\n",
        "avg_losses = []\n",
        "log_span = 500\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epoch):\n",
        "    cumulative_loss = 0\n",
        "    for batch_idx, (context, target) in enumerate(dataloader):\n",
        "        optimiser.zero_grad()\n",
        "        y, correct = model(context, target)\n",
        "        loss = F.binary_cross_entropy(y, correct)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        cumulative_loss += loss.data\n",
        "\n",
        "        if batch_idx % log_span == 0 and batch_idx != 0:\n",
        "          avg_loss = cumulative_loss/log_span\n",
        "          avg_losses.append(avg_loss)\n",
        "          print(\"(\"+str(epoch+1)+\"/\"+str(num_epoch)+\")\", round(100 * batch_idx/num_batches_per_epoch, 2), \"%\")\n",
        "          print(\"       avg loss: \"+str(avg_loss))\n",
        "          cumulative_loss = 0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1/10) 3.84 %\n",
            "       avg loss: 1.9115759113668642\n",
            "(1/10) 7.67 %\n",
            "       avg loss: 1.7710422988155148\n",
            "(1/10) 11.51 %\n",
            "       avg loss: 1.6607071168109897\n",
            "(1/10) 15.34 %\n",
            "       avg loss: 1.5535514232589929\n",
            "(1/10) 19.18 %\n",
            "       avg loss: 1.4531147077515192\n",
            "(1/10) 23.02 %\n",
            "       avg loss: 1.353547775973962\n",
            "(1/10) 26.85 %\n",
            "       avg loss: 1.2583664865234525\n",
            "(1/10) 30.69 %\n",
            "       avg loss: 1.1777139567381907\n",
            "(1/10) 34.53 %\n",
            "       avg loss: 1.1032481575648163\n",
            "(1/10) 38.36 %\n",
            "       avg loss: 1.0332345048604443\n",
            "(1/10) 42.2 %\n",
            "       avg loss: 0.9765158899455155\n",
            "(1/10) 46.03 %\n",
            "       avg loss: 0.9218838343109161\n",
            "(1/10) 49.87 %\n",
            "       avg loss: 0.8768610247172589\n",
            "(1/10) 53.71 %\n",
            "       avg loss: 0.8351331205468342\n",
            "(1/10) 57.54 %\n",
            "       avg loss: 0.8023242806576035\n",
            "(1/10) 61.38 %\n",
            "       avg loss: 0.7678409511140464\n",
            "(1/10) 65.21 %\n",
            "       avg loss: 0.7422213706221715\n",
            "(1/10) 69.05 %\n",
            "       avg loss: 0.7161778740815138\n",
            "(1/10) 72.89 %\n",
            "       avg loss: 0.6902189871027806\n",
            "(1/10) 76.72 %\n",
            "       avg loss: 0.6679451570418763\n",
            "(1/10) 80.56 %\n",
            "       avg loss: 0.6507952262992114\n",
            "(1/10) 84.39 %\n",
            "       avg loss: 0.6305730422130627\n",
            "(1/10) 88.23 %\n",
            "       avg loss: 0.6137389393777236\n",
            "(1/10) 92.07 %\n",
            "       avg loss: 0.6009990840835202\n",
            "(1/10) 95.9 %\n",
            "       avg loss: 0.5856820596760615\n",
            "(1/10) 99.74 %\n",
            "       avg loss: 0.5720592154995999\n",
            "(2/10) 3.84 %\n",
            "       avg loss: 0.5762304889815888\n",
            "(2/10) 7.67 %\n",
            "       avg loss: 0.5294531198945875\n",
            "(2/10) 11.51 %\n",
            "       avg loss: 0.5195688731265606\n",
            "(2/10) 15.34 %\n",
            "       avg loss: 0.5090374525536461\n",
            "(2/10) 19.18 %\n",
            "       avg loss: 0.5008994389442988\n",
            "(2/10) 23.02 %\n",
            "       avg loss: 0.4946429757033079\n",
            "(2/10) 26.85 %\n",
            "       avg loss: 0.48718223796769966\n",
            "(2/10) 30.69 %\n",
            "       avg loss: 0.47856405192368384\n",
            "(2/10) 34.53 %\n",
            "       avg loss: 0.4712470072199214\n",
            "(2/10) 38.36 %\n",
            "       avg loss: 0.4659716002136606\n",
            "(2/10) 42.2 %\n",
            "       avg loss: 0.4576921619596652\n",
            "(2/10) 46.03 %\n",
            "       avg loss: 0.451516231595893\n",
            "(2/10) 49.87 %\n",
            "       avg loss: 0.4459255018756449\n",
            "(2/10) 53.71 %\n",
            "       avg loss: 0.4410896016400452\n",
            "(2/10) 57.54 %\n",
            "       avg loss: 0.4367970993919604\n",
            "(2/10) 61.38 %\n",
            "       avg loss: 0.4334127511070087\n",
            "(2/10) 65.21 %\n",
            "       avg loss: 0.42744191538001336\n",
            "(2/10) 69.05 %\n",
            "       avg loss: 0.42437748048917556\n",
            "(2/10) 72.89 %\n",
            "       avg loss: 0.4182681613886133\n",
            "(2/10) 76.72 %\n",
            "       avg loss: 0.4155113995868651\n",
            "(2/10) 80.56 %\n",
            "       avg loss: 0.41088359893701365\n",
            "(2/10) 84.39 %\n",
            "       avg loss: 0.4091234377699247\n",
            "(2/10) 88.23 %\n",
            "       avg loss: 0.4046274644303826\n",
            "(2/10) 92.07 %\n",
            "       avg loss: 0.40066639500293233\n",
            "(2/10) 95.9 %\n",
            "       avg loss: 0.39763838797079837\n",
            "(2/10) 99.74 %\n",
            "       avg loss: 0.3945596231740183\n",
            "(3/10) 3.84 %\n",
            "       avg loss: 0.3944210247567361\n",
            "(3/10) 7.67 %\n",
            "       avg loss: 0.36522727770108804\n",
            "(3/10) 11.51 %\n",
            "       avg loss: 0.3629765329734946\n",
            "(3/10) 15.34 %\n",
            "       avg loss: 0.3619650097372481\n",
            "(3/10) 19.18 %\n",
            "       avg loss: 0.3595156044120016\n",
            "(3/10) 23.02 %\n",
            "       avg loss: 0.3570778703402428\n",
            "(3/10) 26.85 %\n",
            "       avg loss: 0.354742532959923\n",
            "(3/10) 30.69 %\n",
            "       avg loss: 0.35373513081484564\n",
            "(3/10) 34.53 %\n",
            "       avg loss: 0.3512448634619196\n",
            "(3/10) 38.36 %\n",
            "       avg loss: 0.35028355411708134\n",
            "(3/10) 42.2 %\n",
            "       avg loss: 0.3485023614672665\n",
            "(3/10) 46.03 %\n",
            "       avg loss: 0.3453689736857725\n",
            "(3/10) 49.87 %\n",
            "       avg loss: 0.34509178668425095\n",
            "(3/10) 53.71 %\n",
            "       avg loss: 0.3440808324124495\n",
            "(3/10) 57.54 %\n",
            "       avg loss: 0.34247603608241434\n",
            "(3/10) 61.38 %\n",
            "       avg loss: 0.3405544860601771\n",
            "(3/10) 65.21 %\n",
            "       avg loss: 0.33960919294516334\n",
            "(3/10) 69.05 %\n",
            "       avg loss: 0.33699595404069393\n",
            "(3/10) 72.89 %\n",
            "       avg loss: 0.3356046720870221\n",
            "(3/10) 76.72 %\n",
            "       avg loss: 0.33625283673984707\n",
            "(3/10) 80.56 %\n",
            "       avg loss: 0.33519466757134964\n",
            "(3/10) 84.39 %\n",
            "       avg loss: 0.33218770830951205\n",
            "(3/10) 88.23 %\n",
            "       avg loss: 0.3296268879121471\n",
            "(3/10) 92.07 %\n",
            "       avg loss: 0.32869606908486604\n",
            "(3/10) 95.9 %\n",
            "       avg loss: 0.3263121099438644\n",
            "(3/10) 99.74 %\n",
            "       avg loss: 0.32517000572783555\n",
            "(4/10) 3.84 %\n",
            "       avg loss: 0.32397157267309895\n",
            "(4/10) 7.67 %\n",
            "       avg loss: 0.30189365117273426\n",
            "(4/10) 11.51 %\n",
            "       avg loss: 0.3017312278030056\n",
            "(4/10) 15.34 %\n",
            "       avg loss: 0.30123896589283555\n",
            "(4/10) 19.18 %\n",
            "       avg loss: 0.2996520202309727\n",
            "(4/10) 23.02 %\n",
            "       avg loss: 0.3002745335018947\n",
            "(4/10) 26.85 %\n",
            "       avg loss: 0.2987624432587877\n",
            "(4/10) 30.69 %\n",
            "       avg loss: 0.29876755679717176\n",
            "(4/10) 34.53 %\n",
            "       avg loss: 0.2973500636889196\n",
            "(4/10) 38.36 %\n",
            "       avg loss: 0.2972742812363197\n",
            "(4/10) 42.2 %\n",
            "       avg loss: 0.2952187176734369\n",
            "(4/10) 46.03 %\n",
            "       avg loss: 0.29539961913656215\n",
            "(4/10) 49.87 %\n",
            "       avg loss: 0.29465371053721673\n",
            "(4/10) 53.71 %\n",
            "       avg loss: 0.29349082539064875\n",
            "(4/10) 57.54 %\n",
            "       avg loss: 0.2919860563620161\n",
            "(4/10) 61.38 %\n",
            "       avg loss: 0.2923202296536507\n",
            "(4/10) 65.21 %\n",
            "       avg loss: 0.2912311844295282\n",
            "(4/10) 69.05 %\n",
            "       avg loss: 0.29091041403036444\n",
            "(4/10) 72.89 %\n",
            "       avg loss: 0.2902276056999452\n",
            "(4/10) 76.72 %\n",
            "       avg loss: 0.28988548474696907\n",
            "(4/10) 80.56 %\n",
            "       avg loss: 0.2901948823231542\n",
            "(4/10) 84.39 %\n",
            "       avg loss: 0.2884272833458566\n",
            "(4/10) 88.23 %\n",
            "       avg loss: 0.28858018074654246\n",
            "(4/10) 92.07 %\n",
            "       avg loss: 0.2875863337588938\n",
            "(4/10) 95.9 %\n",
            "       avg loss: 0.28631807168919954\n",
            "(4/10) 99.74 %\n",
            "       avg loss: 0.2856092998170308\n",
            "(5/10) 3.84 %\n",
            "       avg loss: 0.2852511127267634\n",
            "(5/10) 7.67 %\n",
            "       avg loss: 0.2643326989175253\n",
            "(5/10) 11.51 %\n",
            "       avg loss: 0.2648375665278963\n",
            "(5/10) 15.34 %\n",
            "       avg loss: 0.2647123464320569\n",
            "(5/10) 19.18 %\n",
            "       avg loss: 0.2659722535382866\n",
            "(5/10) 23.02 %\n",
            "       avg loss: 0.26469109814142106\n",
            "(5/10) 26.85 %\n",
            "       avg loss: 0.2646554688758\n",
            "(5/10) 30.69 %\n",
            "       avg loss: 0.2635514461895353\n",
            "(5/10) 34.53 %\n",
            "       avg loss: 0.26328036772965824\n",
            "(5/10) 38.36 %\n",
            "       avg loss: 0.2635326334461325\n",
            "(5/10) 42.2 %\n",
            "       avg loss: 0.26268310236325787\n",
            "(5/10) 46.03 %\n",
            "       avg loss: 0.2637309343694763\n",
            "(5/10) 49.87 %\n",
            "       avg loss: 0.2620411038349208\n",
            "(5/10) 53.71 %\n",
            "       avg loss: 0.262779787374507\n",
            "(5/10) 57.54 %\n",
            "       avg loss: 0.26119091874252465\n",
            "(5/10) 61.38 %\n",
            "       avg loss: 0.26060109995425623\n",
            "(5/10) 65.21 %\n",
            "       avg loss: 0.26118017554965045\n",
            "(5/10) 69.05 %\n",
            "       avg loss: 0.2607584108692753\n",
            "(5/10) 72.89 %\n",
            "       avg loss: 0.26217425333081623\n",
            "(5/10) 76.72 %\n",
            "       avg loss: 0.26024342507610343\n",
            "(5/10) 80.56 %\n",
            "       avg loss: 0.26146116572843137\n",
            "(5/10) 84.39 %\n",
            "       avg loss: 0.2605877455064084\n",
            "(5/10) 88.23 %\n",
            "       avg loss: 0.25889200479478397\n",
            "(5/10) 92.07 %\n",
            "       avg loss: 0.2606141538982341\n",
            "(5/10) 95.9 %\n",
            "       avg loss: 0.25835435947052665\n",
            "(5/10) 99.74 %\n",
            "       avg loss: 0.25914607527283995\n",
            "(6/10) 3.84 %\n",
            "       avg loss: 0.25777941426936246\n",
            "(6/10) 7.67 %\n",
            "       avg loss: 0.23949831312682776\n",
            "(6/10) 11.51 %\n",
            "       avg loss: 0.24054809800563054\n",
            "(6/10) 15.34 %\n",
            "       avg loss: 0.241066337568673\n",
            "(6/10) 19.18 %\n",
            "       avg loss: 0.23989699832217773\n",
            "(6/10) 23.02 %\n",
            "       avg loss: 0.24065791882894386\n",
            "(6/10) 26.85 %\n",
            "       avg loss: 0.23995440245137434\n",
            "(6/10) 30.69 %\n",
            "       avg loss: 0.24076520519045513\n",
            "(6/10) 34.53 %\n",
            "       avg loss: 0.24028604209403134\n",
            "(6/10) 38.36 %\n",
            "       avg loss: 0.24112737495219103\n",
            "(6/10) 42.2 %\n",
            "       avg loss: 0.24030828661768663\n",
            "(6/10) 46.03 %\n",
            "       avg loss: 0.23943564737638878\n",
            "(6/10) 49.87 %\n",
            "       avg loss: 0.2400827886163457\n",
            "(6/10) 53.71 %\n",
            "       avg loss: 0.2399148252924372\n",
            "(6/10) 57.54 %\n",
            "       avg loss: 0.2392724415142726\n",
            "(6/10) 61.38 %\n",
            "       avg loss: 0.23973735889898418\n",
            "(6/10) 65.21 %\n",
            "       avg loss: 0.24013469784670832\n",
            "(6/10) 69.05 %\n",
            "       avg loss: 0.23926320033693213\n",
            "(6/10) 72.89 %\n",
            "       avg loss: 0.2405961700502387\n",
            "(6/10) 76.72 %\n",
            "       avg loss: 0.24021895809291852\n",
            "(6/10) 80.56 %\n",
            "       avg loss: 0.23882051815373648\n",
            "(6/10) 84.39 %\n",
            "       avg loss: 0.23999392418572016\n",
            "(6/10) 88.23 %\n",
            "       avg loss: 0.239043522431559\n",
            "(6/10) 92.07 %\n",
            "       avg loss: 0.23947277148890925\n",
            "(6/10) 95.9 %\n",
            "       avg loss: 0.23955374579743366\n",
            "(6/10) 99.74 %\n",
            "       avg loss: 0.23914045331176417\n",
            "(7/10) 3.84 %\n",
            "       avg loss: 0.23880929667355372\n",
            "(7/10) 7.67 %\n",
            "       avg loss: 0.22121119280177728\n",
            "(7/10) 11.51 %\n",
            "       avg loss: 0.22153003938481128\n",
            "(7/10) 15.34 %\n",
            "       avg loss: 0.22262488666896343\n",
            "(7/10) 19.18 %\n",
            "       avg loss: 0.22309671410592624\n",
            "(7/10) 23.02 %\n",
            "       avg loss: 0.22202078110679932\n",
            "(7/10) 26.85 %\n",
            "       avg loss: 0.22288907848786754\n",
            "(7/10) 30.69 %\n",
            "       avg loss: 0.22186602083142978\n",
            "(7/10) 34.53 %\n",
            "       avg loss: 0.22369365655744014\n",
            "(7/10) 38.36 %\n",
            "       avg loss: 0.22319740297618879\n",
            "(7/10) 42.2 %\n",
            "       avg loss: 0.22170147718977606\n",
            "(7/10) 46.03 %\n",
            "       avg loss: 0.2235781562588905\n",
            "(7/10) 49.87 %\n",
            "       avg loss: 0.22305090624146212\n",
            "(7/10) 53.71 %\n",
            "       avg loss: 0.2244359963373436\n",
            "(7/10) 57.54 %\n",
            "       avg loss: 0.22355901891334634\n",
            "(7/10) 61.38 %\n",
            "       avg loss: 0.22356589101698288\n",
            "(7/10) 65.21 %\n",
            "       avg loss: 0.22318483023816066\n",
            "(7/10) 69.05 %\n",
            "       avg loss: 0.22357694870338013\n",
            "(7/10) 72.89 %\n",
            "       avg loss: 0.22404602531549078\n",
            "(7/10) 76.72 %\n",
            "       avg loss: 0.22364759671559115\n",
            "(7/10) 80.56 %\n",
            "       avg loss: 0.22331591976975076\n",
            "(7/10) 84.39 %\n",
            "       avg loss: 0.22412645645440588\n",
            "(7/10) 88.23 %\n",
            "       avg loss: 0.22313134191109119\n",
            "(7/10) 92.07 %\n",
            "       avg loss: 0.22319333492781923\n",
            "(7/10) 95.9 %\n",
            "       avg loss: 0.22353969770995302\n",
            "(7/10) 99.74 %\n",
            "       avg loss: 0.2239551458010977\n",
            "(8/10) 3.84 %\n",
            "       avg loss: 0.2226222712425313\n",
            "(8/10) 7.67 %\n",
            "       avg loss: 0.20781949433303867\n",
            "(8/10) 11.51 %\n",
            "       avg loss: 0.20880301230573575\n",
            "(8/10) 15.34 %\n",
            "       avg loss: 0.2094351928451325\n",
            "(8/10) 19.18 %\n",
            "       avg loss: 0.2078510604788948\n",
            "(8/10) 23.02 %\n",
            "       avg loss: 0.20992568314673768\n",
            "(8/10) 26.85 %\n",
            "       avg loss: 0.20865296779922293\n",
            "(8/10) 30.69 %\n",
            "       avg loss: 0.2095944427260051\n",
            "(8/10) 34.53 %\n",
            "       avg loss: 0.20908404720033857\n",
            "(8/10) 38.36 %\n",
            "       avg loss: 0.2102123226960498\n",
            "(8/10) 42.2 %\n",
            "       avg loss: 0.20992711798822852\n",
            "(8/10) 46.03 %\n",
            "       avg loss: 0.2107021760604913\n",
            "(8/10) 49.87 %\n",
            "       avg loss: 0.2099353794642301\n",
            "(8/10) 53.71 %\n",
            "       avg loss: 0.21055224957212534\n",
            "(8/10) 57.54 %\n",
            "       avg loss: 0.20990759386744612\n",
            "(8/10) 61.38 %\n",
            "       avg loss: 0.21067721309254692\n",
            "(8/10) 65.21 %\n",
            "       avg loss: 0.21095292942059937\n",
            "(8/10) 69.05 %\n",
            "       avg loss: 0.21061224641265422\n",
            "(8/10) 72.89 %\n",
            "       avg loss: 0.2104306320789236\n",
            "(8/10) 76.72 %\n",
            "       avg loss: 0.21086196358952733\n",
            "(8/10) 80.56 %\n",
            "       avg loss: 0.2109751660405425\n",
            "(8/10) 84.39 %\n",
            "       avg loss: 0.2111622162178277\n",
            "(8/10) 88.23 %\n",
            "       avg loss: 0.211875745891035\n",
            "(8/10) 92.07 %\n",
            "       avg loss: 0.21174539067684925\n",
            "(8/10) 95.9 %\n",
            "       avg loss: 0.21025687828685355\n",
            "(8/10) 99.74 %\n",
            "       avg loss: 0.21144551099526787\n",
            "(9/10) 3.84 %\n",
            "       avg loss: 0.21135537729209442\n",
            "(9/10) 7.67 %\n",
            "       avg loss: 0.1973586973068547\n",
            "(9/10) 11.51 %\n",
            "       avg loss: 0.19707554592752247\n",
            "(9/10) 15.34 %\n",
            "       avg loss: 0.19769639295899552\n",
            "(9/10) 19.18 %\n",
            "       avg loss: 0.19803839419821298\n",
            "(9/10) 23.02 %\n",
            "       avg loss: 0.19810692754942869\n",
            "(9/10) 26.85 %\n",
            "       avg loss: 0.19788089535210723\n",
            "(9/10) 30.69 %\n",
            "       avg loss: 0.19885303087952935\n",
            "(9/10) 34.53 %\n",
            "       avg loss: 0.20001821264335562\n",
            "(9/10) 38.36 %\n",
            "       avg loss: 0.19869828655543867\n",
            "(9/10) 42.2 %\n",
            "       avg loss: 0.20024265634787727\n",
            "(9/10) 46.03 %\n",
            "       avg loss: 0.20004906515484838\n",
            "(9/10) 49.87 %\n",
            "       avg loss: 0.19908128628803726\n",
            "(9/10) 53.71 %\n",
            "       avg loss: 0.19970042495314114\n",
            "(9/10) 57.54 %\n",
            "       avg loss: 0.20114313163974987\n",
            "(9/10) 61.38 %\n",
            "       avg loss: 0.20049337086558477\n",
            "(9/10) 65.21 %\n",
            "       avg loss: 0.20016957408507663\n",
            "(9/10) 69.05 %\n",
            "       avg loss: 0.20119098305880975\n",
            "(9/10) 72.89 %\n",
            "       avg loss: 0.20107902008011844\n",
            "(9/10) 76.72 %\n",
            "       avg loss: 0.2008369128410844\n",
            "(9/10) 80.56 %\n",
            "       avg loss: 0.20063160618969922\n",
            "(9/10) 84.39 %\n",
            "       avg loss: 0.20086226366412338\n",
            "(9/10) 88.23 %\n",
            "       avg loss: 0.20110630357349096\n",
            "(9/10) 92.07 %\n",
            "       avg loss: 0.20134889624807445\n",
            "(9/10) 95.9 %\n",
            "       avg loss: 0.20085855552729662\n",
            "(9/10) 99.74 %\n",
            "       avg loss: 0.20158408282771353\n",
            "(10/10) 3.84 %\n",
            "       avg loss: 0.20273635038122642\n",
            "(10/10) 7.67 %\n",
            "       avg loss: 0.18820897507360743\n",
            "(10/10) 11.51 %\n",
            "       avg loss: 0.18935991981023262\n",
            "(10/10) 15.34 %\n",
            "       avg loss: 0.18925338626630156\n",
            "(10/10) 19.18 %\n",
            "       avg loss: 0.18979401623360848\n",
            "(10/10) 23.02 %\n",
            "       avg loss: 0.18978368125477585\n",
            "(10/10) 26.85 %\n",
            "       avg loss: 0.1896025803584149\n",
            "(10/10) 30.69 %\n",
            "       avg loss: 0.19048017646492252\n",
            "(10/10) 34.53 %\n",
            "       avg loss: 0.1905808456317853\n",
            "(10/10) 38.36 %\n",
            "       avg loss: 0.19098241908188665\n",
            "(10/10) 42.2 %\n",
            "       avg loss: 0.19101850454325792\n",
            "(10/10) 46.03 %\n",
            "       avg loss: 0.19021801072152084\n",
            "(10/10) 49.87 %\n",
            "       avg loss: 0.19000713049830906\n",
            "(10/10) 53.71 %\n",
            "       avg loss: 0.19105311240977943\n",
            "(10/10) 57.54 %\n",
            "       avg loss: 0.1916439915368847\n",
            "(10/10) 61.38 %\n",
            "       avg loss: 0.19138052561547086\n",
            "(10/10) 65.21 %\n",
            "       avg loss: 0.192904549135797\n",
            "(10/10) 69.05 %\n",
            "       avg loss: 0.19318429037714355\n",
            "(10/10) 72.89 %\n",
            "       avg loss: 0.19277150966354725\n",
            "(10/10) 76.72 %\n",
            "       avg loss: 0.191746966971393\n",
            "(10/10) 80.56 %\n",
            "       avg loss: 0.19179148569416374\n",
            "(10/10) 84.39 %\n",
            "       avg loss: 0.19336300552548455\n",
            "(10/10) 88.23 %\n",
            "       avg loss: 0.1932548992060876\n",
            "(10/10) 92.07 %\n",
            "       avg loss: 0.19336877955785586\n",
            "(10/10) 95.9 %\n",
            "       avg loss: 0.19319867855731762\n",
            "(10/10) 99.74 %\n",
            "       avg loss: 0.19314612505122625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9T7Fi4pcnGS"
      },
      "source": [
        "# save the embedding, word2id and id2word\n",
        "np.save('w2v_weight.npy',cp.asnumpy(model.emb_in.weight.data))\n",
        "\n",
        "with open('word2id.pkl', 'wb') as handle:\n",
        "    pickle.dump(dataset.corpus.word2id, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('id2word.pkl', 'wb') as handle:\n",
        "    pickle.dump(dataset.corpus.id2word, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9PeSxpWcnNG"
      },
      "source": [
        "# evaluation\n",
        "def most_similar(query, top=5):\n",
        "    word2id = dataset.corpus.word2id\n",
        "    id2word = dataset.corpus.id2word\n",
        "    embedding = model.emb_in.weight.data\n",
        "\n",
        "    if query not in word2id:\n",
        "        print('query not found')\n",
        "        return \n",
        "    \n",
        "    print('query:', query)\n",
        "    \n",
        "    query_id = word2id[query]\n",
        "    query_vec = embedding[query_id]\n",
        "    \n",
        "    vocab_size = len(id2word)\n",
        "    similarity = np.zeros(vocab_size)\n",
        "    for i in range(vocab_size):\n",
        "        similarity[i] = cos_similarity(embedding[i], query_vec)\n",
        "    \n",
        "    count = 0\n",
        "    for i in (-1*similarity).argsort():\n",
        "        if id2word[i] == query:\n",
        "            continue\n",
        "        print(id2word[i], ':', similarity[i])\n",
        "        \n",
        "        count+=1\n",
        "        if count >= top:\n",
        "            return"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "002r8SVseMxo",
        "outputId": "4e9be689-4f8a-4942-e954-d7e9ad2471fa"
      },
      "source": [
        "most_similar('children')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query: children\n",
            "men : 0.5821971893310547\n",
            "people : 0.5753355622291565\n",
            "women : 0.5361747741699219\n",
            "soldiers : 0.5230221152305603\n",
            "civilians : 0.5228411555290222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbgoVjVveM2L",
        "outputId": "2c6a7487-1801-480c-a15d-4b3684fcff24"
      },
      "source": [
        "most_similar('year')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query: year\n",
            "month : 0.6843153238296509\n",
            "years : 0.655612051486969\n",
            "week : 0.5653883814811707\n",
            "months : 0.5553741455078125\n",
            "season : 0.554878830909729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NLZTmbCeM05"
      },
      "source": [
        "# evaluation 2\n",
        "def analogy(a, b, c, top=5):\n",
        "    word2id = dataset.corpus.word2id\n",
        "    id2word = dataset.corpus.id2word\n",
        "    embedding = model.emb_in.weight.data\n",
        "\n",
        "    if a not in word2id or b not in word2id or c not in word2id:\n",
        "        print('query not found')\n",
        "        return \n",
        "    \n",
        "    print('[analogy]', a + ':' + b + ' = ' + c + ':?')\n",
        "    \n",
        "    a_id = word2id[a]\n",
        "    b_id = word2id[b]\n",
        "    c_id = word2id[c]\n",
        "    a_vec = embedding[a_id]\n",
        "    b_vec = embedding[b_id]\n",
        "    c_vec = embedding[c_id]\n",
        "\n",
        "    query_vec = c_vec + b_vec - a_vec\n",
        "    \n",
        "    vocab_size = len(id2word)\n",
        "    similarity = np.zeros(vocab_size)\n",
        "    for i in range(vocab_size):\n",
        "        similarity[i] = cos_similarity(embedding[i], query_vec)\n",
        "    \n",
        "    count = 0\n",
        "    for i in (-1*similarity).argsort():\n",
        "        if id2word[i] == a or id2word[i] == b or id2word[i] == c:\n",
        "            continue\n",
        "        print(id2word[i], ':', similarity[i])\n",
        "        \n",
        "        count+=1\n",
        "        if count >= top:\n",
        "            return"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7nWQ6eFudfj",
        "outputId": "c69ef3a9-db95-4f12-e19f-5b1fb73ca656"
      },
      "source": [
        "analogy('car','cars','child')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[analogy] car:cars = child:?\n",
            "children : 0.481119304895401\n",
            "<n>j<n>qshz : 0.42697370052337646\n",
            "unswervingly : 0.42480939626693726\n",
            "djs : 0.4098614454269409\n",
            "modernizing : 0.4080216884613037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EnggB5exbnk",
        "outputId": "15f38902-a081-4f6a-b1da-e0219dbcbb79"
      },
      "source": [
        "analogy('take','took','go')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[analogy] take:took = go:?\n",
            "went : 0.6760730743408203\n",
            "came : 0.5314589142799377\n",
            "turned : 0.5067143440246582\n",
            "knew : 0.4969695806503296\n",
            "skated : 0.4861728549003601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjPvpLuvxusq",
        "outputId": "d68b273b-9ad1-43ea-8bc2-5edeebb54c28"
      },
      "source": [
        "analogy('good','bad','more')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[analogy] good:bad = more:?\n",
            "less : 0.6630487442016602\n",
            "worse : 0.47593769431114197\n",
            "echte : 0.455361932516098\n",
            "halsted : 0.4298776388168335\n",
            "rather : 0.42482250928878784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUoeH2mEMOJS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}