{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Simple Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parsed_html(url):\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        html = res.text.encode(res.encoding) #prevent encoding errors     \n",
    "        soup = BeautifulSoup(html, 'html.parser') #parse contents 'lxml' or 'html.parser'\n",
    "        return soup\n",
    "    except:\n",
    "        print(\"ERROR: Problem during getting the source or parsing the source\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://google.com/\"\n",
    "soup = get_parsed_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google Search Images Maps Play YouTube News Gmail Drive More »Web History | Settings | Sign in \\xa0Advanced searchGoogle offered in:  Gaeilge Advertising\\xa0ProgramsBusiness SolutionsAbout GoogleGoogle.ie© 2020 - Privacy - Terms '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.html.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Something more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_script(parsed_html):\n",
    "    scripts = parsed_html.find_all('script')\n",
    "    # join all scripts into one string, and remove linebreak\n",
    "    str_scripts = ''.join(map(lambda s: s.text, scripts)).replace('\\n',' ')\n",
    "    # some extraction logic - this should be customised according to the source scripts \n",
    "    ptn_to_extract = '<some regexp to extract from the script here>'\n",
    "    ptn_to_remove = '<some regexp to remove from the result here>'\n",
    "    extracted = ' '.join(re.findall(ptn_to_extract, str_scripts))\n",
    "    extracted = re.sub(ptn_to_remove,'',extracted)\n",
    "    # assume the extracted is now supposed to be a json format\n",
    "    try:\n",
    "        extracted_json = json.loads(extracted)\n",
    "        return extracted_json\n",
    "    except:\n",
    "        print('ERROR: Extracted content is not a json format.')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some configs for your scraping\n",
    "BASE_URL = 'https://www.<path_to_the_page_on_the_website_to_scrape>/?<some_parameter>='\n",
    "PARAMS = [\n",
    "    '<some_parameter_to_run_scrape_1>',\n",
    "    '<some_parameter_to_run_scrape_2>',\n",
    "    '<some_parameter_to_run_scrape_3>',\n",
    "]\n",
    "# interval (seconds) between scrapint trials to avoid too many scrape in a short period.\n",
    "SLEEP_SEC = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping:  https://www.<path_to_the_page_on_the_website_to_scrape>/?<some_parameter>=<some_parameter_to_run_scrape_1>\n",
      "ERROR: Problem during getting the source or parsing the source\n",
      "Failed to scrape : https://www.<path_to_the_page_on_the_website_to_scrape>/?<some_parameter>=<some_parameter_to_run_scrape_1>\n",
      "Scraping:  https://www.<path_to_the_page_on_the_website_to_scrape>/?<some_parameter>=<some_parameter_to_run_scrape_2>\n",
      "ERROR: Problem during getting the source or parsing the source\n",
      "Failed to scrape : https://www.<path_to_the_page_on_the_website_to_scrape>/?<some_parameter>=<some_parameter_to_run_scrape_2>\n",
      "Scraping:  https://www.<path_to_the_page_on_the_website_to_scrape>/?<some_parameter>=<some_parameter_to_run_scrape_3>\n",
      "ERROR: Problem during getting the source or parsing the source\n",
      "Failed to scrape : https://www.<path_to_the_page_on_the_website_to_scrape>/?<some_parameter>=<some_parameter_to_run_scrape_3>\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for param in PARAMS:\n",
    "    time.sleep(SLEEP_SEC)\n",
    "    url = BASE_URL + param\n",
    "    print('Scraping: ',url)\n",
    "    # move on to the next param if an error occurs during the scraping\n",
    "    parsed_html = get_parsed_html(url)\n",
    "    if parsed_html == False:\n",
    "        print('Failed to scrape :',url)\n",
    "        continue\n",
    "    extracted_json = extract_json_from_script(parsed_html)\n",
    "    if extracted_json == False:\n",
    "        print('Failed to scrape :',url)\n",
    "        continue\n",
    "    results[param] = extracted_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
